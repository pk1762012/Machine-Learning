Core classes, functions, and methods:
1. DataPreprocessing - class for preprocessing the data, including creating automated categories and WOE values, calculating Information Value, selecting top variables, and splitting data into train and test sets.
2. XGBoostModel - class for building and tuning the XGBoost model, and evaluating its performance using AUC, confusion matrix, and TDR.
3. Main - entrypoint function to run the entire pipeline.

FILENAME: datapreprocessing.py
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import resample
from scipy.stats import chi2_contingency

class DataPreprocessing:
    def __init__(self, data):
        self.data = data
    
    def create_categories(self, col, num_bins=10):
        """
        Create automated categories/bins for a given column.
        Outliers are assigned to one category, and missing values are assigned another category.
        """
        # Handle missing values
        self.data[col+'_missing'] = np.where(self.data[col].isnull(), 1, 0)
        self.data[col].fillna(-999, inplace=True)
        
        # Handle outliers
        q1 = self.data[col].quantile(0.25)
        q3 = self.data[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5*iqr
        upper_bound = q3 + 1.5*iqr
        self.data[col+'_outlier'] = np.where((self.data[col] < lower_bound) | (self.data[col] > upper_bound), 1, 0)
        self.data[col] = np.where((self.data[col] < lower_bound) | (self.data[col] > upper_bound), -999, self.data[col])
        
        # Create categories
        self.data[col+'_bin'] = pd.qcut(self.data[col], num_bins, duplicates='drop')
        self.data[col+'_bin'] = self.data[col+'_bin'].astype(str)
        
    def calculate_woe(self, col, target):
        """
        Calculate WOE values for each category of a given column.
        """
        total_good = self.data[target].value_counts()[0]
        total_bad = self.data[target].value_counts()[1]
        grouped = self.data.groupby(col+'_bin').agg({target: ['count', 'sum']})
        grouped.columns = ['total', 'bad']
        grouped['good'] = grouped['total'] - grouped['bad']
        grouped['woe'] = np.log((grouped['good']/total_good) / (grouped['bad']/total_bad))
        return grouped['woe'].to_dict()
    
    def calculate_iv(self, col, target):
        """
        Calculate Information Value for a given column.
        """
        total_good = self.data[target].value_counts()[0]
        total_bad = self.data[target].value_counts()[1]
        grouped = self.data.groupby(col+'_bin').agg({target: ['count', 'sum']})
        grouped.columns = ['total', 'bad']
        grouped['good'] = grouped['total'] - grouped['bad']
        grouped['woe'] = np.log((grouped['good']/total_good) / (grouped['bad']/total_bad))
        grouped['iv'] = (grouped['good']/total_good - grouped['bad']/total_bad) * grouped['woe']
        return grouped['iv'].sum()
    
    def select_top_variables(self, target, num_vars=100):
        """
        Select top variables based on Information Value.
        """
        iv = []
        for col in self.data.columns:
            if col != target:
                iv.append((col, self.calculate_iv(col, target)))
        iv_df = pd.DataFrame(iv, columns=['Variable', 'IV'])
        iv_df = iv_df.sort_values('IV', ascending=False).reset_index(drop=True)
        top_vars = iv_df['Variable'][:num_vars]
        self.data = self.data[top_vars.append(pd.Index([target]))]
    
    def split_data(self, target, test_size=0.3):
        """
        Split data into train and test sets.
        """
        X = self.data.drop(target, axis=1)
        y = self.data[target]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
        return X_train, X_test, y_train, y_test
```

FILENAME: xgboostmodel.py
```python
import xgboost as xgb
from sklearn.metrics import roc_auc_score, confusion_matrix

class XGBoostModel:
    def __init__(self, X_train, y_train, X_test, y_test):
        self.X_train = X_train
        self.y_train = y_train
        self.X_test = X_test
        self.y_test = y_test
    
    def hyperparameter_tuning(self):
        """
        Tune hyperparameters of XGBoost model using Bayesian Optimization.
        """
        def xgb_evaluate(max_depth, gamma, colsample_bytree):
            params = {'eval_metric': 'auc',
                      'max_depth': int(max_depth),
                      'subsample': 0.8,
                      'eta': 0.1,
                      'gamma': gamma,
                      'colsample_bytree': colsample_bytree}
            cv_result = xgb.cv(params, xgb.DMatrix(self.X_train, label=self.y_train), num_boost_round=1000, nfold=5)
            return cv_result['test-auc-mean'].iloc[-1]
        
        from bayes_opt import BayesianOptimization
        xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3, 7),
                                                     'gamma': (0, 1),
                                                     'colsample_bytree': (0.3, 0.9)})
        xgb_bo.maximize(init_points=3, n_iter=10)
        params = xgb_bo.max['params']
        params['max_depth'] = int(params['max_depth'])
        self.params = params
    
    def train_model(self):
        """
        Train XGBoost model with GPU mode.
        """
        dtrain = xgb.DMatrix(self.X_train, label=self.y_train)
        dtest = xgb.DMatrix(self.X_test, label=self.y_test)
        self.params['tree_method'] = 'gpu_hist'
        self.params['predictor'] = 'gpu_predictor'
        self.model = xgb.train(self.params, dtrain, num_boost_round=1000, evals=[(dtest, 'test')], early_stopping_rounds=10)
    
    def evaluate_model(self):
        """
        Evaluate XGBoost model using AUC, confusion matrix, and TDR.
        """
        y_pred = self.model.predict(xgb.DMatrix(self.X_test))
        auc = roc_auc_score(self.y_test, y_pred)
        tn, fp, fn, tp = confusion_matrix(self.y_test, y_pred.round()).ravel()
        tdr = tp / (tp + fn)
        return auc, tn, fp, fn, tp, tdr
```

FILENAME: main.py
```python
import pandas as pd
from datapreprocessing import DataPreprocessing
from xgboostmodel import XGBoostModel
import joblib

def main():
    # Load data
    data = pd.read_csv('data.csv')
    
    # Preprocess data
    dp = DataPreprocessing(data)
    for col in data.columns:
        if data[col].dtype == 'object':
            dp.create_categories(col, num_bins=10)
        else:
            dp.create_categories(col, num_bins=20)
    dp.select_top_variables('target', num_vars=100)
    X_train, X_test, y_train, y_test = dp.split_data('target')
    
    # Build and tune XGBoost model
    xgbm = XGBoostModel(X_train, y_train, X_test, y_test)
    xgbm.hyperparameter_tuning()
    xgbm.train_model()
    auc, tn, fp, fn, tp, tdr = xgbm.evaluate_model()
    
    # Save model
    joblib.dump(xgbm.model, 'model.pkl')
    
if __name__ == '__main__':
    main()
```